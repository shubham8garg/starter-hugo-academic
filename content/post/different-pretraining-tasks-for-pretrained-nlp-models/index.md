---
title: Different pretraining tasks for pretrained NLP models.
date: 2022-02-09T03:16:12.421Z
draft: false
featured: false
image:
  filename: featured
  focal_point: Smart
  preview_only: false
---
In recent years, there are a huge number of pretrained NLP models created by researchers. In pretraining the model, a huge amount of data is taken and passed through a model with specific task. These tasks are called as pre-training task. In this article we gonna look at different pre-training tasks:

Depending on whether the pretrained model is encoder only or encoder-decoder framework, there are various pretraining tasks:

1. Masked Language modeling

2. Span Correction

3. Translation Pair Span correction

4. Translation Span Correction

5. Machine Translation





5. Machine Translation

In this task, goal is to translate a sentence from source language to target language